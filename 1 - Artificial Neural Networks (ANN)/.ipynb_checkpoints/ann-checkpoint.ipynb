{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdUFcDsdzRyw"
   },
   "source": [
    "# Clonamos el repositorio para obtener los dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "mHReFf3_y9ms",
    "outputId": "16e47efa-2691-4fcc-992e-e0577a7d0098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'deeplearning-az'...\n",
      "remote: Enumerating objects: 71, done.\u001b[K\n",
      "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
      "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
      "remote: Total 10167 (delta 35), reused 44 (delta 20), pack-reused 10096\u001b[K\n",
      "Receiving objects: 100% (10167/10167), 236.96 MiB | 24.11 MiB/s, done.\n",
      "Resolving deltas: 100% (60/60), done.\n",
      "Checking out files: 100% (10110/10110), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/joanby/deeplearning-az.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNKZXgtKzU2x"
   },
   "source": [
    "# Damos acceso a nuestro Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5gu7KWnzzUQ0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gUxIkHWzfHV"
   },
   "source": [
    "# Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIQt3jBMzYRE"
   },
   "outputs": [],
   "source": [
    "!ls '/content/drive/My Drive' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHsK36uN0XB-"
   },
   "source": [
    "# Google colab tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTzwfUPWzrm4"
   },
   "outputs": [],
   "source": [
    "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
    "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
    "from google.colab import drive # Montar tu Google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uab9OAbV8hYN"
   },
   "source": [
    "# Instalar dependendias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "qukjDgj98kE4",
    "outputId": "ee6f4ce5-2b77-433c-f2bc-29fead6c2ed4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4_lCXxqObOEc"
   },
   "source": [
    "# Instalar Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "id": "ZuKBi9yzbOEd",
    "outputId": "112c8e47-f2d3-4390-d8e4-2bbed8ff91bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/Theano/Theano.git\n",
      "  Cloning git://github.com/Theano/Theano.git to /tmp/pip-req-build-7q89qzfc\n",
      "  Running command git clone -q git://github.com/Theano/Theano.git /tmp/pip-req-build-7q89qzfc\n",
      "Building wheels for collected packages: Theano\n",
      "  Building wheel for Theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for Theano: filename=Theano-1.0.5+1.geb6a4125c-cp36-none-any.whl size=2668281 sha256=4d4c9648f72d9a1cfd6b33dea50b7685d703ab52da5c3aa3fe6ecd7dcad06048\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_6l9lz2i/wheels/ae/32/7c/62beb8371953eb20c271b3bac7d0e56e1a2020d46994346b52\n",
      "Successfully built Theano\n",
      "Installing collected packages: Theano\n",
      "  Found existing installation: Theano 1.0.5\n",
      "    Uninstalling Theano-1.0.5:\n",
      "      Successfully uninstalled Theano-1.0.5\n",
      "Successfully installed Theano-1.0.5+1.geb6a4125c\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQVWgYPgbOEg"
   },
   "source": [
    "# Instalar Tensorflow y Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 768
    },
    "colab_type": "code",
    "id": "lQlELtH5bOEg",
    "outputId": "430a74ac-094d-4a8b-935c-7dd333751fe8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.35.1)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (49.6.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yFpBwmNz70v"
   },
   "source": [
    "# Redes Neuronales Artificales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8OxSXXSz-OP"
   },
   "source": [
    "# Parte 0 - Cómo importar las librerías y el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edZX51YLzs59"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XfXlqtF0B58"
   },
   "source": [
    "# Importar el data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nnozsHsz_-N"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtzD5VD4bOEn"
   },
   "source": [
    "# Parte 1 - Pre procesado de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsVEdPzf4XmV"
   },
   "source": [
    "## Codificar datos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9CtwK834bjy"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = ColumnTransformer(\n",
    "    [('one_hot_encoder', OneHotEncoder(categories='auto'), [1])],   \n",
    "    remainder='passthrough'                        \n",
    ")\n",
    "X = onehotencoder.fit_transform(X)\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AH_uCEz68rb"
   },
   "source": [
    "## Dividir el data set en conjunto de entrenamiento y conjunto de testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeuAy8LI69vi"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfzLXHPwbOEt"
   },
   "source": [
    "## Escalado de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8m2LCgz8bOEt"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cgweTaJ67BOB",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Parte 2 - Construir la RNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tm4Hg995bOEw"
   },
   "source": [
    "## Importar Keras y librerías adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLEt5ni_bOEw"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezfPlyegbOEy"
   },
   "source": [
    "## Inicializar la RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zRmQjNVzbOEz"
   },
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ML7H-iqsbOE1"
   },
   "source": [
    "## Añadir las capas de entrada y primera capa oculta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "amkkZxnebOE1"
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 6, kernel_initializer = \"uniform\",  \n",
    "                     activation = \"relu\", input_dim = 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKoC5UOwbOE3"
   },
   "source": [
    "## Añadir la segunda capa oculta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZTDhh_JibOE3"
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 6, kernel_initializer = \"uniform\",  activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvUfptEtbOE5"
   },
   "source": [
    "## Añadir la capa de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6C2x9RUGbOE5"
   },
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 1, kernel_initializer = \"uniform\",  activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Je4JFqsbOE7"
   },
   "source": [
    "## Compilar la RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f89xXBswbOE7"
   },
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dr34zpQrbOE9"
   },
   "source": [
    "## Ajustamos la RNA al Conjunto de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bwNUtZp7bOE9",
    "outputId": "d21731d6-ef82-47ed-bf24-1b22d0df0b4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 2s 1ms/step - loss: 0.4791 - accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4283 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4228 - accuracy: 0.8004\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4185 - accuracy: 0.8230\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4163 - accuracy: 0.8269\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4142 - accuracy: 0.8285\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4129 - accuracy: 0.8313\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4114 - accuracy: 0.8319\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4102 - accuracy: 0.8324\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4093 - accuracy: 0.8344\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4087 - accuracy: 0.8349\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4080 - accuracy: 0.8344\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4073 - accuracy: 0.8345\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4068 - accuracy: 0.8334\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4060 - accuracy: 0.8342\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4058 - accuracy: 0.8328\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4051 - accuracy: 0.8339\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4052 - accuracy: 0.8341\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4046 - accuracy: 0.8345\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4042 - accuracy: 0.8354\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4040 - accuracy: 0.8354\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4034 - accuracy: 0.8341\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4032 - accuracy: 0.8345\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4033 - accuracy: 0.8339\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4031 - accuracy: 0.8329\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4031 - accuracy: 0.8340\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.8339\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4024 - accuracy: 0.8340\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4027 - accuracy: 0.8354\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4028 - accuracy: 0.8342\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.8339\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.8335\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8344\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8359\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4022 - accuracy: 0.8342\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4019 - accuracy: 0.8359\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4022 - accuracy: 0.8340\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4023 - accuracy: 0.8357\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8344\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4011 - accuracy: 0.8339\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4019 - accuracy: 0.8345\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4016 - accuracy: 0.8356\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4016 - accuracy: 0.8360\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4015 - accuracy: 0.8336\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4017 - accuracy: 0.8359\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4013 - accuracy: 0.8338\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4015 - accuracy: 0.8355\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4013 - accuracy: 0.8353\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4013 - accuracy: 0.8350\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4014 - accuracy: 0.8336\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8370\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4016 - accuracy: 0.8355\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8366\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4011 - accuracy: 0.8357\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4012 - accuracy: 0.8346\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4011 - accuracy: 0.8326\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4015 - accuracy: 0.8341\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4010 - accuracy: 0.8349\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8351\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8341\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4013 - accuracy: 0.8359\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8349\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8347\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8350\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4010 - accuracy: 0.8342\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4010 - accuracy: 0.8350\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8350\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8353\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4014 - accuracy: 0.8353\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8334\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4010 - accuracy: 0.8351\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8356\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8355\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4007 - accuracy: 0.8356\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4007 - accuracy: 0.8341\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4010 - accuracy: 0.8342\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8347\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8344\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4011 - accuracy: 0.8330\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8340\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4001 - accuracy: 0.8345\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4011 - accuracy: 0.8350\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4010 - accuracy: 0.8354\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4012 - accuracy: 0.8347\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8359\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4007 - accuracy: 0.8354\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4009 - accuracy: 0.8342\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4010 - accuracy: 0.8326\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8355\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4007 - accuracy: 0.8353\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8331\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8349\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8350\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8355\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8338\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4006 - accuracy: 0.8344\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4010 - accuracy: 0.8346\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8341\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 1s 2ms/step - loss: 0.4000 - accuracy: 0.8326\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 1s 1ms/step - loss: 0.4005 - accuracy: 0.8340\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d5b1b23050>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train,  batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jsUJ7kBnbOE_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Parte 3 - Evaluar el modelo y calcular predicciones finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elaboramos una predicción para un cliente particular con la ANN entrenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0, 0.0, 619, 0, 42, 2, 0.0, 1, 1, 1, 101348.88], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver en dataset que valor corresponde a cada columna, y si es necesario o no (si eliminamos su ficticia ponemos 0,0 como en el caso de Francia)\n",
    "dataset # vemos que el primer dato es francés y mujer\n",
    "dataset.iloc[0,:].values\n",
    "X[0,:] # vemos que el francés posee 2 ceros en las primeras 2 columnas (correspondientes a 0 en la columna de Alemania y de España, ya que eliminamos justo la ficticia de Francia)\n",
    "# también vemos del mismo que posee un 0 en la columna de género, por lo que 0 es para mujer y 1 para hombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n",
      "[[0.09149386]]\n",
      "[[False]]\n"
     ]
    }
   ],
   "source": [
    "#Utiliza nuestro modelo de RNA para predecir si el cliente con la siguiente información abandonará el banco:\n",
    "\n",
    "#Geografia: Francia\n",
    "#Puntaje de crédito: 600\n",
    "#Género masculino\n",
    "#Edad: 40 años de edad\n",
    "#Tenencia: 3 años.\n",
    "#Saldo: $ 60000\n",
    "#Número de productos: 2\n",
    "#¿Este cliente tiene una tarjeta de crédito? Sí\n",
    "#¿Es este cliente un miembro activo? Sí\n",
    "#Salario estimado: $ 50000\n",
    "#Entonces, ¿deberíamos decir adiós a ese cliente?\n",
    "\n",
    "new_prediction = classifier.predict(sc_X.transform(np.array([[0,0,600, 1, 40, 3, 60000, 2, 1, 1, 50000]]))) # acordarse que convertimos Geografía y Género. Introducimos los datos con la transformación/escalado de sc_X, que me transforma las entradas en los datos escalados que espera la red\n",
    "print(new_prediction)\n",
    "print(new_prediction > 0.5) # clasifica según el umbral del 50% en True (abandona) o False (no abandona)\n",
    "\n",
    "# El resultado es False, por lo que, con el 7% de probabilidad el cliente se irá del banco, por lo que no le tendremos que decir adios si nuestro criterio es despedir a quien supere el 50%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ZWV9kiBbOFA"
   },
   "source": [
    "## Predicción de los resultados con el Conjunto de Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVzX_pIhbOFA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred  = classifier.predict(X_test)\n",
    "y_pred = (y_pred>0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0R6AD0bbOFD"
   },
   "source": [
    "## Elaborar una matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cv-193GvbOFD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1545   50]\n",
      " [ 260  145]]\n",
      "0.845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm) \n",
    "print((cm[0][0]+cm[1][1])/cm.sum()) # Accuracy de la NN con respecto a los datos de test (no se produce Overffiting, ya que es similar la precisión sobre el conjunto de train y sobre el de test. Si fuera mucho mayor en train que en test si)\n",
    "# Forma de evaluar las predicciones van a ser las mismas que en los algoritmos de ML\n",
    "# Comparar resultado de predicción sobre test con el conseguido sobre train en el entrenamiento de la red para ver si hay Overffiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qBAu2KobrIh",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Parte 4 - Evaluar, mejorar y Ajustar la RNA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3by0rPFbuM3"
   },
   "source": [
    "## Evaluar la **RNA**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall keras -y\n",
    "#!pip install keras==2.12.0\n",
    "#!pip install tensorflow==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xn44nPWXbsbS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score # validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEv9LR5MbwJ_"
   },
   "outputs": [],
   "source": [
    "# Definimos una función que cree la red neuronal con KerasClassifier. El reescalado me hará falta en el contexto del k-fold, no en esta etapa.\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout # Dropout, se aplica para evitar sobreajuste luego de cada capa que requiera aprender (en nuestro problema no es necesario, ya que hay poca varianza en las iteraciones del k-fold)\n",
    "\n",
    "def build_classifier():\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(units = 6, kernel_initializer = \"uniform\", activation = \"relu\", input_dim = 11))\n",
    "  #classifier.add(Dropout(p = 0.1)) # p: probabilidad de ser desactivadas las neuronas (si tenemos 10 y p=0.1, 1 de las 10 neuronas será desactivada por iteración), aumentar p cuando veamos que no se resuelve el Overffiting (sin pasarnos de 0.5 para no entrar en Underffiting)\n",
    "  classifier.add(Dense(units = 6, kernel_initializer = \"uniform\", activation = \"relu\"))\n",
    "  #classifier.add(Dropout(p = 0.1))\n",
    "  classifier.add(Dense(units = 1, kernel_initializer = \"uniform\", activation = \"sigmoid\"))\n",
    "  classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "  return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "84ER1wwKbx-s",
    "outputId": "6c5a2563-275c-426c-f7e3-0c20708acd05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxi Mores\\AppData\\Local\\Temp\\ipykernel_4032\\702602064.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, nb_epoch = 100) # clasificador de keras. build_fn: función de generación de arquitectura de red, batch_size y nb_epoch: tamaño del bloque que una vez finalizado su procesado se actualizan pesos y el número de épocas (igual que antes)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:    4.2s remaining:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    6.5s finished\n"
     ]
    }
   ],
   "source": [
    "# Creamos el clasificador con Keras (hacer de cuenta que el código empieza en la parte 4)\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, nb_epoch = 100) # clasificador de keras. build_fn: función de generación de arquitectura de red, batch_size y nb_epoch: tamaño del bloque que una vez finalizado su procesado se actualizan pesos y el número de épocas (igual que antes)\n",
    "\n",
    "# En nuestro caso, vamos a aplicar antes la división en train/test (opcional) y el reescalado de las variables (X_train y Y_train si optamos por div por % antes o sobre X y Y si no)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "# Aplicamos validación cruzada (k-fold) a la ANN en vez de división en train/test almacenando los resultados en un vector de precisiones llamado 'accuracies'\n",
    "accuracies = cross_val_score(estimator=classifier, X = X_train, y = y_train, cv = 10, n_jobs=-1, verbose = 1) # estimator: objeto para ajustar los datos, X y Y: las VI y VD de para aplicar CV, cv: número de folds a utilizar (generalmente se usa 10, siendo un buen criterio de uso en términos de eficacia/rapidez), si no nos da buenos resultados podemos optar por un número mayor de folds, n_jobs: -1 para utilizar todos los procesadores de las máquinas en paralelo (usamos toda la potencia del ordenador), \n",
    "# El entrenamiento se realizará tomando como train los 9 folds de la iteración, y el restante como test, sobre el X y Y aclarado\n",
    "# En vez de hacerlo con X_train y Y_train lo podemos hacer con todo el set de datos directamente, poniendo X y Y, pero con esta opción nos guardamos una parte de las variables para luego ver como el modelo reacciona ante otros datos no usados para el entrenamiento con kfolds (a pesar de que cada resultado de fold es válido), siendo necesario de aplicar en este caso la división en train/test antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78625   , 0.79000002, 0.80000001, 0.78250003, 0.81625003,\n",
       "       0.81      , 0.78750002, 0.79374999, 0.79874998, 0.79500002])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies # vemos las precisiones obtenidas en cada iteración\n",
    "# Rendimientos inferiores a las obtenidas anteriormente solo con train/test, pero con poca varianza (indicio de que la partición específica usada en train/test estaba por encima del rendimiento promedio del modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kH5SWOLjbz1J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de rendimiento sobre el set entero: 0.7960000097751617\n",
      "Desviación std de las 10 iteraciones sobre el set entero: 0.010105690527913859\n"
     ]
    }
   ],
   "source": [
    "mean = accuracies.mean() \n",
    "variance = accuracies.std()\n",
    "print(f'Media de rendimiento sobre el set de train: {mean}')\n",
    "print(f'Desviación std de las 10 iteraciones sobre el set de train: {variance}')\n",
    "# La precisión media es el 80% aprox, con una varianza del 1,2%. A lo mejor podríamos exigirle a la ANN un poco más de precisión, pero no varianza\n",
    "# Intentemos buscar una red que llegue al 85-86 % de rendimiento, ya que esta media nos indica la precisión en el entrenamiento validado sobre datos no vistos durante la iteración, osea que es un rendimiento válido (se hace sobre datos no usados para entrenamiento, solo que en vez de usar todo el dataset usamos solo la parte de train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mv-Hs66Tb3Mv"
   },
   "source": [
    "## Mejorar la RNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-TdOlIlb22x"
   },
   "source": [
    "#### Regularización de Dropout para evitar el *overfitting*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El sobreajuste es cuando nuestro modelo aprende demasiado bien sobre los datos de entrenamiento, faltandolé efectividad al momento de enfrentarse con datos nuevos (test)\n",
    "# Podriamos identificar un caso de Overffiting en una ANN notando una gran varianza en las iteraciones del k-fold validation, ya que las correlaciones que el modelo aprende muy bien de los folds de entrenamiento no son aplicables al fold de test de similar manera en cada iteración, dependiendo más así de la división específica de los folds\n",
    "# Si el desempeño de cada iteración es similar, es síntoma de que el rendimiento del modelo no depende tanto de la asignación de datos en folds, siendo capaz de captar patrones/tendencias aplicables sobre datos no usados para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tenemos mucha varianza en las iteraciones debemos corregirla con la capa Dropout. Esta capa descativa aleatoriamente neuronas de la capa anterior en cada iteración, para que las mismas no se especialicen sobre los mismos rasgos (se sobreescriban) y no sean demasiado dependientes\n",
    "# Esto nos garantiza que las correlaciones que aprenda cada neurona serán más independientes una de la otra \n",
    "# Dropout se puede aplicar luego de cualquier capa con neuronas que deban aprender. Conviene aplciarlo después de todas las capas, e ir incrementando la cantidad de capas Dropout y la probabilidad de desactivarse a medida de que veamos que no soluciona el Overffiting\n",
    "# También conviene tener muchas capas de Dropout con bajo valor p despues de cada capa densa, en vez de tener solo una con un valor p alto (fuerte prob de olvido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k0xNpbSSb7_D"
   },
   "source": [
    "## Ajustar la *RNA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscaremos mejorar el rendimiento de la red con Grid Search, buscando los hiperparámetros óptimos de la arquitectura para encontrar el mejor rendimiento\n",
    "# Grid Search nos devolverá la mejor combinación de hiperparámetros para conseguir el rendimiento más elevado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4XOkEVJ_eE93"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lURNzjkSb1Ce"
   },
   "outputs": [],
   "source": [
    "# Volvemos a definir la arquitectura de la ANN, no agregamos el optimizador, el cual vamos a incluir en la búsqueda de hiperparámetros óptimos\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(units = 6, kernel_initializer = \"uniform\",  activation = \"relu\", input_dim = 11))\n",
    "  classifier.add(Dense(units = 6, kernel_initializer = \"uniform\",  activation = \"relu\"))\n",
    "  classifier.add(Dense(units = 1, kernel_initializer = \"uniform\",  activation = \"sigmoid\"))\n",
    "  classifier.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])  \n",
    "  return classifier\n",
    "\n",
    "# Si queremos buscar un hiperparámetro presente en la arquitectura, se lo otorgamos a la función como parámetro y ponemos la variable en donde se pide el hiperparámetro. Podemos agregar varios parámetros, separándolos por comas en los argumentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1mRpi1Kcb96V"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxi Mores\\AppData\\Local\\Temp\\ipykernel_4032\\1443411995.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  classifier = KerasClassifier(build_fn = build_classifier)\n"
     ]
    }
   ],
   "source": [
    "# Ahora al classifier no le aclaramos ni el batch_size ni el número de épocas, ya que esos serán algunos de los hiperparámetros que buscaremos el óptimo\n",
    "classifier = KerasClassifier(build_fn = build_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkRJLSzAb_6R"
   },
   "outputs": [],
   "source": [
    "# Hacemos una lista de los hiperparámetros que queremos optimizar seguida de una lista de los valores que queremos que pruebe (ojo con poner muchas combinaciones)\n",
    "parameters = {\n",
    "    'batch_size' : [25,32], # mientras mayor es el bloque, más datos pasan por la ANN antes de actualizar los pesos\n",
    "    'nb_epoch' : [100, 500], \n",
    "    'optimizer' : ['adam', 'rmsprop']\n",
    "}\n",
    "# Podemos optimizar:\n",
    "# - el tamaño del bloque (batch_size) ---> probar con potencias de 2\n",
    "# - el número de épocas (nb_epoch) ---> probar más épocas a ver si llegamos a convergencia\n",
    "# - el número de neuronas de las capas (units)\n",
    "# - el tipo de kernel (kernel_initializer) \n",
    "# - la función de activación (activation) \n",
    "# - la métrica (no tan relevante)\n",
    "# - el optimizador (optimizer) ---> rmsprop esta basado en el gradiente descendiente y es recomendado para ANN regulares (mientras que adam es el recomendado para ANN más complejas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 922
    },
    "colab_type": "code",
    "id": "_Ejn37PBcB-7",
    "outputId": "608bcae3-2311-4466-b4f1-4f4998a075db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288/288 [==============================] - 1s 1ms/step - loss: 0.6098 - accuracy: 0.7947\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 2s 1ms/step - loss: 0.6331 - accuracy: 0.7943\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.5787 - accuracy: 0.7940\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5652 - accuracy: 0.7975\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.6078 - accuracy: 0.7928\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5961 - accuracy: 0.7924\n",
      "25/25 [==============================] - 0s 2ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.6027 - accuracy: 0.7957\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5896 - accuracy: 0.7950\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.6096 - accuracy: 0.7947\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.6089 - accuracy: 0.7954\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 2ms/step - loss: 0.5778 - accuracy: 0.7965\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 2s 2ms/step - loss: 0.5940 - accuracy: 0.7951\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5945 - accuracy: 0.7933\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5983 - accuracy: 0.7961\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5856 - accuracy: 0.7932\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5763 - accuracy: 0.7942\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.6033 - accuracy: 0.7961\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 993us/step - loss: 0.5655 - accuracy: 0.7962\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5774 - accuracy: 0.7944\n",
      "25/25 [==============================] - 0s 875us/step\n",
      "288/288 [==============================] - 1s 1ms/step - loss: 0.5805 - accuracy: 0.7950\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.5937 - accuracy: 0.7958\n",
      "25/25 [==============================] - 0s 875us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6464 - accuracy: 0.7946\n",
      "25/25 [==============================] - 0s 875us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6510 - accuracy: 0.7915\n",
      "25/25 [==============================] - 0s 917us/step\n",
      "225/225 [==============================] - 1s 996us/step - loss: 0.6124 - accuracy: 0.7958\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6228 - accuracy: 0.7907\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6308 - accuracy: 0.7935\n",
      "25/25 [==============================] - 0s 917us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.5912 - accuracy: 0.7969\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6040 - accuracy: 0.7957\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6336 - accuracy: 0.7926\n",
      "25/25 [==============================] - 0s 875us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6165 - accuracy: 0.7949\n",
      "25/25 [==============================] - 0s 833us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6027 - accuracy: 0.7956\n",
      "25/25 [==============================] - 0s 875us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.5989 - accuracy: 0.7968\n",
      "25/25 [==============================] - 0s 833us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.5968 - accuracy: 0.7954\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6180 - accuracy: 0.7965\n",
      "25/25 [==============================] - 0s 792us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6038 - accuracy: 0.7925\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6089 - accuracy: 0.7933\n",
      "25/25 [==============================] - 0s 833us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6477 - accuracy: 0.7940\n",
      "25/25 [==============================] - 0s 1000us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.5979 - accuracy: 0.7961\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6265 - accuracy: 0.7943\n",
      "25/25 [==============================] - 0s 1ms/step\n",
      "225/225 [==============================] - 1s 1ms/step - loss: 0.6094 - accuracy: 0.7947\n",
      "25/25 [==============================] - 0s 958us/step\n",
      "320/320 [==============================] - 1s 1ms/step - loss: 0.5623 - accuracy: 0.7960\n"
     ]
    }
   ],
   "source": [
    "# Usamos GridSearchCV para combinar Cross Validation y la búsqueda de hiperparámetros configurada en parameters\n",
    "grid_search = GridSearchCV(estimator = classifier, \n",
    "                           param_grid = parameters, \n",
    "                           scoring = 'accuracy', # indicamos la medida con la que queremos comparar los resultados de combinación de parámetros (ver que hay más parámetros)\n",
    "                           cv = 10)\n",
    "\n",
    "grid_search = grid_search.fit(X_train, y_train) # usamos el objeto para que se ajuste a los datos de entrenamiento\n",
    "\n",
    "# (investigar más propiedades/atributos del objeto de grid_search)\n",
    "best_parameters = grid_search.best_params_ # mejor selección de parámetros\n",
    "best_accuracy = grid_search.best_score_ # resultado obtenido (según la métrica especificada) con la mejor combinación de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'batch_size': 25, 'nb_epoch': 100, 'optimizer': 'rmsprop'}\n",
      "Mejor accuracy (métrica): 0.796\n"
     ]
    }
   ],
   "source": [
    "print(f'Mejores parámetros: {best_parameters}')\n",
    "print(f'Mejor accuracy (métrica): {best_accuracy}')\n",
    "# Vemos que, como fueron pocas la combinaciones de hiperparámetros que pedimos y no fueron las mejores opciones a buscar, pero podemos y debemos seguir buscando"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5 - Obtener mejores resultados con Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score # validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a definir la arquitectura de la ANN, no agregamos el optimizador, el cual vamos a incluir en la búsqueda de hiperparámetros óptimos\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "  classifier = Sequential()\n",
    "  classifier.add(Dense(units = 22, kernel_initializer = \"uniform\",  activation = \"relu\", input_dim = 11))\n",
    "  classifier.add(Dense(units = 22, kernel_initializer = \"uniform\",  activation = \"relu\"))\n",
    "  classifier.add(Dense(units = 11, kernel_initializer = \"uniform\",  activation = \"relu\")) # tercera capa oculta\n",
    "  classifier.add(Dense(units = 1, kernel_initializer = \"uniform\",  activation = \"sigmoid\"))\n",
    "  classifier.compile(optimizer = optimizer, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])  \n",
    "  return classifier\n",
    "\n",
    "# Si queremos buscar un hiperparámetro presente en la arquitectura, se lo otorgamos a la función como parámetro y ponemos la variable en donde se pide el hiperparámetro. Podemos agregar varios parámetros, separándolos por comas en los argumentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maxi Mores\\AppData\\Local\\Temp\\ipykernel_14136\\1443411995.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  classifier = KerasClassifier(build_fn = build_classifier)\n"
     ]
    }
   ],
   "source": [
    "# Ahora al classifier no le aclaramos ni el batch_size ni el número de épocas, ya que esos serán algunos de los hiperparámetros que buscaremos el óptimo\n",
    "classifier = KerasClassifier(build_fn = build_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos una lista de los hiperparámetros que queremos optimizar seguida de una lista de los valores que queremos que pruebe (ojo con poner muchas combinaciones)\n",
    "parameters = {\n",
    "    'batch_size' : [10, 20], # mientras mayor es el bloque, más datos pasan por la ANN antes de actualizar los pesos\n",
    "    'nb_epoch' : [ 20, 30, 50], \n",
    "    'optimizer' : ['adam', 'rmsprop'],\n",
    "    'validation_split': [0.1] # porcentaje de los datos que guardaremos para validar el entrenamiento y evitar el overffiting\n",
    "}\n",
    "# multiplicar opciones para ver si el número de combinaciones no es muy elevado\n",
    "\n",
    "# Podemos optimizar:\n",
    "# - el tamaño del bloque (batch_size) ---> probar con potencias de 2\n",
    "# - el número de épocas (nb_epoch) ---> probar más épocas a ver si llegamos a convergencia\n",
    "# - el número de neuronas de las capas (units)\n",
    "# - el tipo de kernel (kernel_initializer) \n",
    "# - la función de activación (activation) \n",
    "# - la métrica (no tan relevante)\n",
    "# - el optimizador (optimizer) ---> rmsprop esta basado en el gradiente descendiente y es recomendado para ANN regulares (mientras que adam es el recomendado para ANN más complejas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la parte 0 y parte 1 (Obtención de Datos y Preprocesado de Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720/720 [==============================] - 2s 2ms/step - loss: 0.4661 - accuracy: 0.7954 - val_loss: 0.4156 - val_accuracy: 0.7950\n"
     ]
    }
   ],
   "source": [
    "# Usamos GridSearchCV para combinar Cross Validation y la búsqueda de hiperparámetros configurada en parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(estimator = classifier, \n",
    "                           param_grid = parameters, \n",
    "                           scoring = 'accuracy', # indicamos la medida con la que queremos comparar los resultados de combinación de parámetros (ver que hay más parámetros)\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "\n",
    "grid_search = grid_search.fit(X_train, y_train) # usamos el objeto para que se ajuste a los datos de entrenamiento\n",
    "\n",
    "# (investigar más propiedades/atributos del objeto de grid_search)\n",
    "best_parameters = grid_search.best_params_ # mejor selección de parámetros\n",
    "best_accuracy = grid_search.best_score_ # resultado obtenido (según la métrica especificada) con la mejor combinación de hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'batch_size': 10, 'nb_epoch': 50, 'optimizer': 'adam', 'validation_split': 0.1}\n",
      "Mejor accuracy (métrica): 0.797625\n"
     ]
    }
   ],
   "source": [
    "print(f'Mejores parámetros: {best_parameters}')\n",
    "print(f'Mejor accuracy (métrica): {best_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ann.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
